{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the scikit learn blog on Text Feature Extraction\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectoroizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create corpus(a list of text documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Thrice to thine',\n",
    "    'and Thrice to mine',\n",
    "    'and thrice again to make it nine'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'again'), (1, 'and'), (2, 'it'), (3, 'make'), (4, 'mine'), (5, 'nine'), (6, 'thine'), (7, 'thrice'), (8, 'to')]\n",
      "[[0 0 0 0 0 0 1 1 1]\n",
      " [0 1 0 0 1 0 0 1 1]\n",
      " [1 1 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus) # learn the vocabulary in the corpus\n",
    "\n",
    "print(list(enumerate(vectorizer.get_feature_names()))) # get the vocabulary\n",
    "print(vectorizer.transform(corpus).toarray()) # DTM: Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer :  ['What', 'goes', 'around', 'comes', 'around']\n",
      "preprocessor :  what goes around comes around\n",
      "analyser :  ['what goes', 'goes around', 'around comes', 'comes around']\n"
     ]
    }
   ],
   "source": [
    "bi_gram_Vector = CountVectorizer(ngram_range=(2, 2))\n",
    "bi_gram_document_term_matrix = bi_gram_Vector.fit_transform(corpus)\n",
    "\n",
    "analyser = bi_gram_Vector.build_analyzer()\n",
    "preprocessor = bi_gram_Vector.build_preprocessor()\n",
    "tokenizer = bi_gram_Vector.build_tokenizer()\n",
    "\n",
    "text = \"What goes around comes around\"\n",
    "\n",
    "print('tokenizer : ', tokenizer(text))\n",
    "print('preprocessor : ', preprocessor(text))\n",
    "print('analyser : ', analyser(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'again to'), (1, 'and thrice'), (2, 'it nine'), (3, 'make it'), (4, 'thrice again'), (5, 'thrice to'), (6, 'to make'), (7, 'to mine'), (8, 'to thine')]\n",
      "[[0 0 0 0 0 1 0 0 1]\n",
      " [0 1 0 0 0 1 0 1 0]\n",
      " [1 1 1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(bi_gram_Vector.get_feature_names()))) # get the vocabulary\n",
    "print(bi_gram_Vector.transform(corpus).toarray()) # DTM: Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.76749457, 0.45329466, 0.45329466],\n",
       "       [0.        , 0.50410689, 0.        , 0.        , 0.66283998,\n",
       "        0.        , 0.        , 0.39148397, 0.39148397],\n",
       "       [0.43535684, 0.3311001 , 0.43535684, 0.43535684, 0.        ,\n",
       "        0.43535684, 0.        , 0.25712876, 0.25712876]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "tf_idf = pipeline.fit_transform(corpus)\n",
    "tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('again', 1.6931471805599454),\n",
       " ('and', 1.2876820724517808),\n",
       " ('it', 1.6931471805599454),\n",
       " ('make', 1.6931471805599454),\n",
       " ('mine', 1.6931471805599454),\n",
       " ('nine', 1.6931471805599454),\n",
       " ('thine', 1.6931471805599454),\n",
       " ('thrice', 1.0),\n",
       " ('to', 1.0)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pipeline.named_steps.countvectorizer.get_feature_names()\n",
    "idf = pipeline.named_steps.tfidftransformer.idf_\n",
    "set(zip(features, idf)) # idf = ln((n+1)/(df(t)+1)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF if smoothen was false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('again', 2.09861228866811),\n",
       " ('and', 1.4054651081081644),\n",
       " ('it', 2.09861228866811),\n",
       " ('make', 2.09861228866811),\n",
       " ('mine', 2.09861228866811),\n",
       " ('nine', 2.09861228866811),\n",
       " ('thine', 2.09861228866811),\n",
       " ('thrice', 1.0),\n",
       " ('to', 1.0)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps.tfidftransformer.set_params(smooth_idf = False)\n",
    "tf_idf = pipeline.fit_transform(corpus)\n",
    "\n",
    "features = pipeline.named_steps.countvectorizer.get_feature_names()\n",
    "idf = pipeline.named_steps.tfidftransformer.idf_\n",
    "set(zip(features, idf)) # idf = ln(n/df(t)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.829279  , 0.39515588, 0.39515588],\n",
       "       [0.        , 0.48552418, 0.        , 0.        , 0.72497497,\n",
       "        0.        , 0.        , 0.34545446, 0.34545446],\n",
       "       [0.45163284, 0.30246377, 0.45163284, 0.45163284, 0.        ,\n",
       "        0.45163284, 0.        , 0.21520547, 0.21520547]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF without Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 2.09861229, 1.        , 1.        ],\n",
       "       [0.        , 1.40546511, 0.        , 0.        , 2.09861229,\n",
       "        0.        , 0.        , 1.        , 1.        ],\n",
       "       [2.09861229, 1.40546511, 2.09861229, 2.09861229, 0.        ,\n",
       "        2.09861229, 0.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps.tfidftransformer.set_params(norm='')\n",
    "tf_idf = pipeline.fit_transform(corpus)\n",
    "tf_idf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with L1 Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.51202996, 0.24398502, 0.24398502],\n",
       "       [0.        , 0.25534981, 0.        , 0.        , 0.38128321,\n",
       "        0.        , 0.        , 0.18168349, 0.18168349],\n",
       "       [0.17784979, 0.11910808, 0.17784979, 0.17784979, 0.        ,\n",
       "        0.17784979, 0.        , 0.08474638, 0.08474638]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps.tfidftransformer.set_params(norm='l1')\n",
    "tf_idf = pipeline.fit_transform(corpus)\n",
    "tf_idf.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizers in TD-IDF\n",
    "### L2:  **(default)** \n",
    "*Formula* = $ \\frac{v_i}{\\sqrt\\Sigma(v_i^2)}$\n",
    "\n",
    "### L1:\n",
    "*Formula* = $ \\frac{v_i}{\\Sigma |v_i|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Vectorizer\n",
    "\n",
    "It is a combination of the Count Vectorizer and the TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT NUMBER  0\n",
      "\t thine      : 0.7674945674619879\n",
      "\t thrice     : 0.4532946552278861\n",
      "\t to         : 0.4532946552278861\n",
      "DOCUMENT NUMBER  1\n",
      "\t and        : 0.5041068915759233\n",
      "\t mine       : 0.6628399823470976\n",
      "\t thrice     : 0.39148397136265967\n",
      "\t to         : 0.39148397136265967\n",
      "DOCUMENT NUMBER  2\n",
      "\t again      : 0.43535684236960664\n",
      "\t and        : 0.33110010014200913\n",
      "\t it         : 0.43535684236960664\n",
      "\t make       : 0.43535684236960664\n",
      "\t nine       : 0.43535684236960664\n",
      "\t thrice     : 0.25712876433201076\n",
      "\t to         : 0.25712876433201076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vectoriser = TfidfVectorizer()\n",
    "document_term_matrix = tf_idf_vectoriser.fit_transform(corpus)\n",
    "features = tf_idf_vectoriser.get_feature_names()\n",
    "\n",
    "for document in list(enumerate(document_term_matrix.toarray())):\n",
    "    print(\"DOCUMENT NUMBER \", document[0])\n",
    "    for token, tf_idf in zip(features, document[1]):\n",
    "        if(tf_idf > 0):\n",
    "            print(f'\\t {token:10} : {tf_idf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar count may be obtained for 2-5 gram words with no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT NUMBER  0\n",
      "\t thrice to  : 1.2876820724517808\n",
      "\t thrice to thine : 1.6931471805599454\n",
      "\t to thine   : 1.6931471805599454\n",
      "DOCUMENT NUMBER  1\n",
      "\t and thrice : 1.2876820724517808\n",
      "\t and thrice to : 1.6931471805599454\n",
      "\t and thrice to mine : 1.6931471805599454\n",
      "\t thrice to  : 1.2876820724517808\n",
      "\t thrice to mine : 1.6931471805599454\n",
      "\t to mine    : 1.6931471805599454\n",
      "DOCUMENT NUMBER  2\n",
      "\t again to   : 1.6931471805599454\n",
      "\t again to make : 1.6931471805599454\n",
      "\t again to make it : 1.6931471805599454\n",
      "\t again to make it nine : 1.6931471805599454\n",
      "\t and thrice : 1.2876820724517808\n",
      "\t and thrice again : 1.6931471805599454\n",
      "\t and thrice again to : 1.6931471805599454\n",
      "\t and thrice again to make : 1.6931471805599454\n",
      "\t it nine    : 1.6931471805599454\n",
      "\t make it    : 1.6931471805599454\n",
      "\t make it nine : 1.6931471805599454\n",
      "\t thrice again : 1.6931471805599454\n",
      "\t thrice again to : 1.6931471805599454\n",
      "\t thrice again to make : 1.6931471805599454\n",
      "\t thrice again to make it : 1.6931471805599454\n",
      "\t to make    : 1.6931471805599454\n",
      "\t to make it : 1.6931471805599454\n",
      "\t to make it nine : 1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectoriser = TfidfVectorizer(ngram_range=(2, 5), norm='')\n",
    "document_term_matrix = tf_idf_vectoriser.fit_transform(corpus)\n",
    "features = tf_idf_vectoriser.get_feature_names()\n",
    "\n",
    "for document in list(enumerate(document_term_matrix.toarray())):\n",
    "    print(\"DOCUMENT NUMBER \", document[0])\n",
    "    for token, tf_idf in zip(features, document[1]):\n",
    "        if(tf_idf > 0):\n",
    "            print(f'\\t {token:10} : {tf_idf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "# Text Processing Notes\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "- Algorithms require numerical feature vector\n",
    "- Convert raw string to numerical feature vector : vectorization\n",
    "    - Bag of n grams:\n",
    "        - Tokenization: convert string to tokens\n",
    "        - Counting: count the number of tokens in each vector\n",
    "        - Normalization: give weight to the tokens present in the document\n",
    "- Steps in vectorizer class:\n",
    "    - Preprocessor: clean the string like remove html tags and convert all to lowercase\n",
    "    - Tokenizer: preprocessor outputs => tokens.\n",
    "    - Analyzer: by default calls the preprocessor, tokenizer and does n-gram extraction and stop word filtering.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "Count Vectorizer:\n",
    "- Text document to matrix of token count\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "TFIDF Transformer\n",
    "- Count matrix => TF-IDF representation\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "Pipelines in skit learn\n",
    "- Pipeline has steps\n",
    "- Each step has a name and an estimator object.\n",
    "- Pipeline is a list of tuple of name string and estimator object\n",
    "- Calling fit on the pipeline is same as calling fit and transform on each and every estimator in the pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
